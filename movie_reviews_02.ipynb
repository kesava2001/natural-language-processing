{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie_reviews_02.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "surAiFPJY396",
        "outputId": "6e01e74d-846a-4c65-e1d4-f0ac8468d42a"
      },
      "source": [
        "import os, re, string, numpy\n",
        "import nltk\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi5tN1sPagDV"
      },
      "source": [
        "def load_docs(filename):\n",
        "  file = open(filename, 'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_doc(filename):\n",
        "  text = load_docs(filename)\n",
        "  tokens = text.split()\n",
        "  tokens = [t.lower() for t in tokens]\n",
        "  tokens = [re.sub('[^a-zA-Z]', ' ', t) for t in tokens]\n",
        "  tokens = [t for t in tokens if t.isalpha()]\n",
        "  sw = set(stopwords.words('english'))\n",
        "  tokens = [t for t in tokens if t not in sw]\n",
        "  tokens = [t for t in tokens if len(t)>1]\n",
        "  return tokens"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJDz5GE3yo2c"
      },
      "source": [
        "def doc_to_line(filename, vocab):\n",
        "  tokens = clean_doc(filename)\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "def process(filename, vocab, istrain):\n",
        "  lines = list()\n",
        "  for dir in os.listdir(filename):\n",
        "    if istrain and dir.startswith('cv9'):\n",
        "      continue\n",
        "    if not istrain and not dir.startswith('cv9'):\n",
        "      continue\n",
        "    path = filename + '/' + dir\n",
        "    line = doc_to_line(path, vocab)\n",
        "    lines.append(line)\n",
        "  return lines"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFjDO68YZJ9T"
      },
      "source": [
        "def load_clean_dataset(vocab, istrain, n_dir, p_dir):\n",
        "  neg = process(n_dir, vocab, istrain)\n",
        "  pos = process(p_dir, vocab, istrain)\n",
        "  docs = neg+pos\n",
        "  labels = [0 for _ in range(len(neg))]+[1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNlb6XMI18FM"
      },
      "source": [
        "def create_model( n_words):\n",
        "  model = keras.models.Sequential([\n",
        "                                   keras.layers.Dense(50, input_shape=(n_words,), activation='relu'),\n",
        "                                   keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jAzTbj1rPic"
      },
      "source": [
        "def clean_review(review):\n",
        "  tokens = review.split()\n",
        "  tokens = [t.lower() for t in tokens]\n",
        "  tokens = [re.sub('[^a-zA-Z]', ' ', t) for t in tokens]\n",
        "  tokens = [t for t in tokens if t.isalpha()]\n",
        "  sw = set(stopwords.words('english'))\n",
        "  tokens = [t for t in tokens if t not in sw]\n",
        "  tokens = [t for t in tokens if len(t)>1]\n",
        "  return tokens\n",
        "\n",
        "def predict_sentiment(model, vocab, tokenizer, review):\n",
        "  tokens = clean_review(review)\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  line = ' '.join(tokens)\n",
        "  encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "  y = model.predict(encoded, verbose=0)\n",
        "  pos_percentage = y[0, 0]\n",
        "  if(round(pos_percentage)==0):\n",
        "    return 1-pos_percentage, 'NEGATIVE'\n",
        "  return pos_percentage, 'POSITIVE'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcj_QeZVcuFd"
      },
      "source": [
        "n_dir = '/content/drive/MyDrive/Colab Notebooks/NLP/data sets/txt_sentoken/neg'\n",
        "p_dir = '/content/drive/MyDrive/Colab Notebooks/NLP/data sets/txt_sentoken/pos'\n",
        "v_dir = '/content/drive/MyDrive/Colab Notebooks/NLP/data sets/txt_sentoken/vocab.txt'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypd4h29Od_T-"
      },
      "source": [
        "vocab = load_docs(v_dir)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True, n_dir, p_dir)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False, n_dir, p_dir)\n",
        "\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "X_train = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
        "X_test = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
        "\n",
        "ytrain = numpy.array(ytrain)\n",
        "ytesy = numpy.array(ytest)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I6q2upz4CaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711aeadf-55e6-4b4e-9f5b-9fa7079a91b7"
      },
      "source": [
        "n_words = X_train.shape[1]\n",
        "model = create_model(n_words)\n",
        "model.fit(X_train, ytrain, epochs=10, verbose=2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "57/57 - 1s - loss: 0.4960 - accuracy: 0.7672\n",
            "Epoch 2/10\n",
            "57/57 - 1s - loss: 0.0894 - accuracy: 0.9856\n",
            "Epoch 3/10\n",
            "57/57 - 0s - loss: 0.0246 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "57/57 - 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "57/57 - 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "57/57 - 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "57/57 - 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "57/57 - 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "57/57 - 0s - loss: 9.8731e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "57/57 - 0s - loss: 7.6204e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe65fd34c50>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_thpHVAHtfuI",
        "outputId": "d22f0fbe-eade-4cd4-9c20-36c0ee9de501"
      },
      "source": [
        "p_review = 'best movie ever, it was great. i definitely recomend it'\n",
        "percent, senti = predict_sentiment(model, vocab, tokenizer, p_review)\n",
        "print('review: [%s] \\nSentiment: %s (%.3f%%)'%(p_review, senti, percent*100))\n",
        "\n",
        "n_review = 'this movie is so boring'\n",
        "per, sen = predict_sentiment(model, vocab, tokenizer, n_review)\n",
        "print('review: [%s] \\nSentiment: %s (%.3f%%)'%(n_review, sen, per*100))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review: [best movie ever, it was great. i definitely recomend it] \n",
            "Sentiment: POSITIVE (61.225%)\n",
            "review: [this movie is so boring] \n",
            "Sentiment: NEGATIVE (66.642%)\n"
          ]
        }
      ]
    }
  ]
}